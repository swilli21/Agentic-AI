{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ew4v0Xw5-5zk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13b2d0e-117a-47bb-e597-5cea0d63a5c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/155.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/155.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.1/500.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#  Install required packages\n",
        "!pip install -qU langchain langchain-google-genai google-generativeai==0.8.5 3 # connect langchain to Gemini model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08RTcrbcy8yC",
        "outputId": "66ca70b1-9bc7-43d3-a9c9-8b75674bacf2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.11)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.6.9)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (9.1.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.14.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, platform\n",
        "from google.colab import userdata\n",
        "from datetime import datetime\n",
        "\n",
        "# LangChain + Gemini\n",
        "import langchain, langchain_core\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI       # to work with Gemini models\n",
        "from langchain_core.prompts import PromptTemplate               # to format prompts\n",
        "from langchain_core.output_parsers import StrOutputParser       # to extract plain text from responses\n",
        "from langchain_core.runnables import RunnablePassthrough        # to chain steps"
      ],
      "metadata": {
        "id": "iCP375sp_HoE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Set your Gemini API key\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GOOGLE_GEMINI_API\")"
      ],
      "metadata": {
        "id": "VvMj21Hu_WVn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommended for demos: fast & economical\n",
        "model_name = \"gemini-2.5-flash\"\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=model_name,\n",
        "    convert_system_message_to_human=True,  # smoother behavior with system prompts\n",
        "    temperature=0.7, #more creative response\n",
        ")\n",
        "print(f\" Ready with model: {model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwF7vVBO_4j2",
        "outputId": "c4fbc264-7311-49e7-f331-9a97ca436f09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ready with model: gemini-2.5-flash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First LLM call.\n",
        "msg = \"Explain AI in one sentence for a complete beginner.\"\n",
        "resp = llm.invoke(msg)   # ChatModels accept a plain string or list of messages\n",
        "print(resp.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PPJHiDC__kF",
        "outputId": "f7d9b97f-79ce-47f2-bd9c-eb3e0a41be1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI is when computers are programmed to learn and perform tasks that typically require human intelligence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PromptTemplate → Chain (Runnable)"
      ],
      "metadata": {
        "id": "yxRn2Q9hALwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a motivational quote generator with a parameterized prompt\n",
        "motivation_prompt = PromptTemplate.from_template(\n",
        "    \"Write a short, motivational quote for someone working as a {profession}. \" #parameterized prompt that takes in a profession\n",
        "    \"Keep it under 20 words.\"\n",
        ")"
      ],
      "metadata": {
        "id": "5P-YQgSsAIxQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1\n",
        "\n",
        "*   Format the prompt with the input, then send it to the LLM\n",
        "*   Extract the plain text from the response\n",
        "\n",
        "LangChain connects components in a pipeline"
      ],
      "metadata": {
        "id": "69uZMIu2UsPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain: Prompt → LLM → String output\n",
        "chain = motivation_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "gLw9gpWHARFI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the prompt by passing a profession"
      ],
      "metadata": {
        "id": "uC_7zzXbjWBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Example (profession=teacher):\")\n",
        "print(chain.invoke({\"profession\": \"teacher\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-ymlq_VAUil",
        "outputId": "02c2d97f-99b6-4771-f0f9-5fa595c95110"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Example (profession=teacher):\n",
            "The future is built in your classroom. Keep empowering minds!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick Playground helper"
      ],
      "metadata": {
        "id": "IVtCxnBwHMRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(prompt: str):\n",
        "    \"\"\"Mini helper to chat with the model.\"\"\"\n",
        "    res = llm.invoke(prompt)\n",
        "    return res.content\n",
        "\n",
        "print(ask(\"Give me 3 icebreakers to start a workshop on AI for Beginners.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5AZKU1wHQCs",
        "outputId": "373395a8-9dc0-42cb-c37d-6adf2066d915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, here are 3 icebreaker ideas for a workshop on AI for Beginners, designed to be engaging and non-intimidating:\n",
            "\n",
            "**1.  \"AI in Your Life\" (Simple & Relatable)**\n",
            "\n",
            "*   **How it works:** Ask participants to turn to a neighbor and briefly share one example of AI they encountered in the last 24 hours. It could be anything - a recommendation on a streaming service, a spam filter, voice assistant, navigation app, etc.\n",
            "*   **Why it works:**\n",
            "    *   **Low pressure:**  Everyone has likely encountered AI recently, making it easy to participate.\n",
            "    *   **Relatable:**  It grounds the abstract concept of AI in everyday experiences.\n",
            "    *   **Quick:**  Keeps the momentum going.\n",
            "    *   **Highlights AI's pervasiveness:** Shows that AI isn't some futuristic concept, but something already integrated into their lives.\n",
            "*   **Facilitator Follow-up:** After a minute or two, ask a few volunteers to share what they discussed.  You can then use these examples to springboard into a brief overview of different types of AI.\n",
            "\n",
            "**2. \"AI Association Game\" (Creative & Thought-Provoking)**\n",
            "\n",
            "*   **How it works:**  Write the letters \"A\" and \"I\" on a whiteboard or flip chart.  Ask participants to brainstorm words or phrases that come to mind when they hear \"AI\" for each of the letters.  (e.g., A - Automation, Algorithms, Amazing;  I - Intelligence, Innovation, Impact).  You can have them shout out responses, or use a collaborative online tool like Mentimeter or a shared document.\n",
            "*   **Why it works:**\n",
            "    *   **Activates Prior Knowledge:** Taps into existing (even if limited) knowledge about AI.\n",
            "    *   **Reveals Expectations & Concerns:**  Helps you gauge the audience's current understanding and potential anxieties about AI.\n",
            "    *   **Visual & Collaborative:**  The visual aspect of writing down the ideas makes it more engaging.\n",
            "    *   **Fun & Fast-Paced:** Can be done quickly and with a bit of playful energy.\n",
            "*   **Facilitator Follow-up:**  Summarize the key themes that emerged.  Acknowledge any misconceptions or concerns.  Use the words and phrases to frame the workshop content.\n",
            "\n",
            "**3. \"AI Wish List\" (Future-Oriented & Engaging)**\n",
            "\n",
            "*   **How it works:** Ask participants to individually write down one thing they *wish* AI could do to make their life easier, better, or more interesting.  Then, have them share their wish with the group (either by reading it aloud or posting it on a virtual whiteboard like Miro or Jamboard).\n",
            "*   **Why it works:**\n",
            "    *   **Positive and Imaginative:**  Focuses on the potential benefits of AI, rather than the potential risks.\n",
            "    *   **Personal & Relevant:**  Connects AI to their individual needs and desires.\n",
            "    *   **Generates Enthusiasm:**  Gets people excited about the possibilities of AI.\n",
            "    *   **Good Discussion Starter:**  The wish list can lead to discussions about the feasibility of different AI applications.\n",
            "*   **Facilitator Follow-up:**  Group the wishes into common themes (e.g., healthcare, education, entertainment).  Discuss which wishes are already being addressed by AI and which are still aspirational. This leads nicely into talking about current AI capabilities and limitations.\n",
            "\n",
            "**Key Considerations for Choosing an Icebreaker:**\n",
            "\n",
            "*   **Time:**  Keep it short (5-10 minutes max).\n",
            "*   **Audience Size:**  Choose an activity that scales well to the number of participants.\n",
            "*   **Technical Setup:**  Consider whether you'll be in-person or virtual and choose an activity that works in that environment.\n",
            "*   **Your Style:** Pick an icebreaker that you feel comfortable facilitating and that aligns with your overall workshop style.\n",
            "*   **Purpose:**  Make sure the icebreaker connects to the workshop's learning objectives in some way.\n",
            "\n",
            "Good luck with your AI workshop!\n"
          ]
        }
      ]
    }
  ]
}