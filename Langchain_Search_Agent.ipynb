{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Search Tools"
      ],
      "metadata": {
        "id": "C8eQDMavPEuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides the framework for agents and tools\n",
        "\n",
        "The langchain-google-genai connector lets us use Gemini models, and the google-generativeai SDK enables communication with Gemini itself.\n",
        "\n",
        "From the wifipedia package, we pull in the interface for Wikipedia queries. From LangChain, we import the tool decorator the Gemini Chat model wrapper PromptTemplates and the agent utilities to tie everything together.\n",
        "\n",
        "Finally, we set the wikipedia language to Englsih, so that all results come back in a consistent format.\n",
        "\n",
        "_format_hits takes the raw search results from DuckDuckGo and formats them into a clean list. It loops through the first few results, pulls out the title, url and snippet, then arranges them in a numbered format.\n",
        "If no results are found, it simply returns No results.\n",
        "raw API responses arent always learner-friendly\n",
        "We often need a formatter to present them clearly to the agent or to the user.\n",
        "\n",
        "web_search - runs a DuckDuckGo search for any query returning a compact list of results with titles, URLs and snippets.\n",
        "We can also set how many results to return and even filter by recency,like the past day, week, month or year. In the function we call the DuckDuckGo client, collect the hits, and format them neatly using our helper.\n",
        "If something goes wrong, the tool returns a clear error message instead of crashing.\n",
        "This is how you wrap a search API, into a reusable LangChain tool, giving our agent the ability to look up live information on demand.\n",
        "\n",
        "wiki_summary it takes a topic name and returns a short Wikipedia summary along with the page URL. Inside, it first loads the page, then extracts a few sentences from the summary. If the topic is ambiguous, it warns the user and shows some options.\n",
        "If the page doesnt exist, it returns a clear message, and if any other error happens, it also handled gracefully.\n",
        "Wraps Wikipedia into a reliable knowledge source for our agent, so it can pull quick trustworthy summaries when users ask about a topic."
      ],
      "metadata": {
        "id": "SwbrNZlzCYDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J7MQllSfOsb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044d1cd6-266f-4601-a912-04e3dd03e126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/155.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.1/500.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install packages\n",
        "!pip install -qU langchain langchain-google-genai google-generativeai==0.8.5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU ddgs # duck duck go library;"
      ],
      "metadata": {
        "id": "efaPibK7O2V-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50eb566-8b03-457c-f70d-f51934955f0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/161.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/4.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DcAxpRi_Oax",
        "outputId": "618aec0e-b4f9-4bfe-cfb6-34cb7f802f78"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-classic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7Ufk9XF_cc8",
        "outputId": "1184b40d-94dc-4580-8095-40597a5f667c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_GEMINI_API\")  # paste your key\n",
        "print(\" API key set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYXfhezwO2zZ",
        "outputId": "94c761b5-7b11-4f49-f333-01d662124f9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " API key set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define tools (DuckDuckGo + Wikipedia)"
      ],
      "metadata": {
        "id": "IKwxZtF8PCHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from ddgs import DDGS\n",
        "import wikipedia\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI       # to work with Gemini models\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "wikipedia.set_lang(\"en\")\n",
        "\n",
        "def _format_hits(hits: List[dict], k: int = 5) -> str:\n",
        "    lines = []\n",
        "    for i, h in enumerate(hits[:k], 1):\n",
        "        title = h.get(\"title\") or h.get(\"heading\") or \"\"\n",
        "        url = h.get(\"href\") or h.get(\"url\") or \"\"\n",
        "        snippet = h.get(\"body\") or h.get(\"description\") or h.get(\"snippet\") or \"\"\n",
        "        lines.append(f\"{i}. {title}\\n   {url}\\n   {snippet}\")\n",
        "    return \"\\n\".join(lines) if lines else \"No results.\"\n",
        "\n",
        "@tool\n",
        "def web_search(query: str, max_results: int = 5, time_range: str = None) -> str:\n",
        "    \"\"\"DuckDuckGo web search. Returns up to `max_results` compact results with URLs and snippets.\n",
        "    time_range can be 'd','w','m','y' (day, week, month, year) for recency, or None.\"\"\"\n",
        "    try:\n",
        "        with DDGS() as ddg:\n",
        "            hits = list(ddg.text(query, max_results=max_results, timelimit=time_range))\n",
        "        return _format_hits(hits, k=max_results)\n",
        "    except Exception as e:\n",
        "        return f\"Search error: {e}\"\n",
        "\n",
        "@tool\n",
        "def wiki_summary(topic: str, sentences: int = 3) -> str:\n",
        "    \"\"\"Return a short Wikipedia summary with the canonical page URL.\"\"\"\n",
        "    try:\n",
        "        page = wikipedia.page(topic, auto_suggest=True, redirect=True)\n",
        "        summary = wikipedia.summary(page.title, sentences=sentences)\n",
        "        return f\"Title: {page.title}\\nURL: {page.url}\\nSummary: {summary}\"\n",
        "    except wikipedia.DisambiguationError as e:\n",
        "        opts = \"; \".join(e.options[:5])\n",
        "        return f\"Disambiguation: be specific. Options include: {opts}\"\n",
        "    except wikipedia.PageError:\n",
        "        return \"No matching page found.\"\n",
        "    except Exception as e:\n",
        "        return f\"Wikipedia error: {e}\""
      ],
      "metadata": {
        "id": "v7Or2kaHO-dN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the search agent\n",
        "\n",
        "- build_search_agent() - Then, we register the two tools we created web_search for fresh or news like queries and wiki_summary for background and definitions.\n",
        "\n",
        "Next,we define a custom system prompt\n",
        "it tells the agent to always use tools for facts, numbers or dates and explains when to choose search vs Wikipedia.\n",
        "It also reminds the agent to cite sources with titles and URLs keep answers concise, and never hallucinate.\n",
        "\n",
        "create a tool_calling_agent with these rules and wrap it in Executor which gives us a simple .invoke interface\n",
        "\n",
        "At the bottom, we call the function to initialize the agent and print a message confirming its ready.\n",
        "Combining multiple tools under one agent and guiding it with a well designed prompt, we can create a reliable, research assistant that choose the right source for the right job."
      ],
      "metadata": {
        "id": "_XksHvzNPWm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.agents import AgentExecutor, create_tool_calling_agent\n",
        "def build_search_agent():\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2)\n",
        "    tools = [web_search, wiki_summary]\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\n",
        "         \"You are a research assistant.\\n\"\n",
        "         \"Use tools for ANY factual claims, numbers, dates, or 'latest' info.\\n\"\n",
        "         \"Choose:\\n\"\n",
        "         \"- web_search: for fresh/news or when URLs are required.\\n\"\n",
        "         \"- wiki_summary: for background/definitions.\\n\"\n",
        "         \"When you cite, include page titles and URLs from tool outputs.\\n\"\n",
        "         \"Keep answers concise. Prefer top 3 results. Do not hallucinate.\"\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ])\n",
        "\n",
        "    agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "    return AgentExecutor(agent=agent, tools=tools, verbose=False)  # clean output\n",
        "\n",
        "search_agent = build_search_agent()\n",
        "print(\" Search agent ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMskscr8PTgS",
        "outputId": "ac2635ba-2603-4986-e373-b7202ce98bdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Search agent ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function called ask that takes in a question, sends it to the agent, and prints the output"
      ],
      "metadata": {
        "id": "Xmi_-S7gRp0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick tests"
      ],
      "metadata": {
        "id": "9yIYxTzaPePr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(q: str):\n",
        "    res = search_agent.invoke({\"input\": q})\n",
        "    print(res[\"output\"], \"\\n\")\n",
        "\n",
        "# 1) Simple fact + citation\n",
        "ask(\"Who won the FIFA World Cup 2022? Give one sentence and include a source URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY2eVEaRPcVr",
        "outputId": "eaa48a66-ea22-4632-86d5-9e7be8587220"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'type': 'text', 'text': 'Argentina won the FIFA World Cup 2022, defeating France in the final. Source: https://en.wikipedia.org/wiki/2022_', 'extras': {'signature': 'CiQBvj72+8ZeajAOv+y9DiVH/XchxbisAtqyo9yYYGcYhVMSRMsKbQG+Pvb7DJcch/ZwGkpevAGYmhkMfrL5Htu55OVWBzrFDIW4DpscQ8bFhmCE+u0SAOxJg4JOOVdk9j4CiO/a9iuczJLfp+nFXqvDCVKcVUlwIJbGvzfx43nME+x0Ed/vREPE+DL9rT2qeUVL9m8KWAG+Pvb70UiHp/AXTZjrfPU9sqt6etWLwOPunYjCcLPXPZaUZ2x9Q7sTs7WUIXscR1pXYweclClQIAZ6fs/jXByU4BBojDxSns9QOZkwt/VIgCzjymvhYNo='}, 'index': 0}, 'FIFA_World_Cup'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Fresh headlines (limit and summarize)\n",
        "ask(\"Find 3 recent headlines about generative AI this week. One line per item with URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXJMoBWUR93I",
        "outputId": "8508851c-f699-4265-f2a0-20543c977440"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 3 recent headlines about generative AI:\n",
            "\n",
            "1.  Free AI Headline Generator | Create Catchy Headlines Instantly\n",
            "    https://www.grammarly.com/ai/ai-writing-tools/headline-generator\n",
            "2.  Generative AI Headlines - GINGER LIU\n",
            "    https://www.gingerliu.com/generative-ai-headlines-3/\n",
            "3.  Listen to Generative AI Headlines 4–4–2\n",
            "    https://www.linkedin.com/pulse/listen-generative-ai-headlines-442-ginger-liu-m-f-a--ero9e \n",
            "\n"
          ]
        }
      ]
    }
  ]
}